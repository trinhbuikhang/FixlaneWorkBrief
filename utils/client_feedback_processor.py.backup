"""
Client Feedback Processor for Data Processing Tool
"""

import polars as pl
import logging
from typing import Optional, Callable
from pathlib import Path

logger = logging.getLogger(__name__)


class ClientFeedbackProcessor:
    """Handles client feedback processing using Polars."""

    def __init__(self, progress_callback: Optional[Callable] = None):
        """
        Initialize client feedback processor.

        Args:
            progress_callback: Optional callback function for progress updates
        """
        self.progress_callback = progress_callback

    def _emit_progress(self, message: str, progress: float = None):
        """Emit progress update if callback is available."""
        if self.progress_callback:
            self.progress_callback(message, progress)
        logger.info(message)

    def _validate_file_exists(self, file_path: str) -> bool:
        """Validate that file exists."""
        import os
        if not os.path.exists(file_path):
            error_msg = f"ERROR: File not found: {file_path}"
            logger.error(error_msg)
            self._emit_progress(error_msg)
            return False
        return True

    def _standardize_boolean_columns(self, df: pl.DataFrame, boolean_columns: list) -> pl.DataFrame:
        """Standardize boolean columns to True/False values."""
        for col in boolean_columns:
            if col in df.columns:
                df = df.with_columns([
                    pl.when(pl.col(col).cast(pl.Utf8).str.strip_chars().is_in(['1', 'True', 'true', 'TRUE', 'T', 't']))
                    .then(True)
                    .when(pl.col(col).cast(pl.Utf8).str.strip_chars().is_in(['0', 'False', 'false', 'FALSE', 'F', 'f', '']))
                    .then(False)
                    .otherwise(None)
                    .alias(col)
                ])
        return df

    def _prepare_for_csv_output(self, df: pl.DataFrame) -> pl.DataFrame:
        """Prepare dataframe for CSV output with proper boolean and null handling."""
        # Convert all columns to string to preserve original formatting
        expressions = []

        for col in df.columns:
            if df[col].dtype == pl.Boolean:
                # Convert boolean columns to True/False strings (with capital T/F)
                expr = (
                    pl.when(pl.col(col).is_null())
                    .then(pl.lit(""))  # Empty string for null values
                    .when(pl.col(col))
                    .then(pl.lit("True"))  # Capital T
                    .otherwise(pl.lit("False"))  # Capital F
                    .alias(col)
                )
                expressions.append(expr)
            else:
                # Convert all other columns to string to preserve formatting
                expr = pl.col(col).cast(pl.Utf8).alias(col)
                expressions.append(expr)

        df = df.with_columns(expressions)
        return df

    def process_in_memory(self, lmd_df: pl.DataFrame, feedback_path: str) -> Optional[pl.DataFrame]:
        """
        Process client feedback and update LMD data in memory using Polars.

        Args:
            lmd_df: Polars DataFrame containing LMD data
            feedback_path: Path to client feedback CSV file

        Returns:
            Updated Polars DataFrame if successful, None otherwise
        """
        try:
            self._emit_progress("Starting client feedback processing...")

            # Load feedback data
            feedback_df = pl.read_csv(feedback_path, ignore_errors=True, infer_schema_length=0)
            if feedback_df is None or len(feedback_df) == 0:
                self._emit_progress("ERROR: Failed to load client feedback data")
                return None

            # Process feedback data
            result_df = self._process_client_feedback_data_polars(lmd_df, feedback_df)

            if result_df is None:
                self._emit_progress("ERROR: Client feedback processing failed")
                return None

            self._emit_progress("Client feedback processing completed")
            return result_df

        except Exception as e:
            error_msg = f"Client feedback processing failed: {str(e)}"
            logger.error(f"{error_msg}\n{e}", exc_info=True)
            self._emit_progress(error_msg)
            return None

    def process_with_chunking(self, lmd_path: str, feedback_path: str, chunk_size: int = 50000) -> Optional[pl.DataFrame]:
        """
        Process client feedback with chunking for large files.

        Args:
            lmd_path: Path to LMD input CSV file
            feedback_path: Path to client feedback CSV file
            chunk_size: Number of rows to process per chunk

        Returns:
            Updated Polars DataFrame if successful, None otherwise
        """
        try:
            self._emit_progress("Starting client feedback processing with chunking...")

            # Validate files exist
            if not self._validate_file_exists(lmd_path) or not self._validate_file_exists(feedback_path):
                return None

            # Load feedback data (keep in memory as it's usually smaller)
            feedback_df = pl.read_csv(feedback_path, ignore_errors=True, infer_schema_length=0)
            if feedback_df is None or len(feedback_df) == 0:
                self._emit_progress("ERROR: Failed to load client feedback data")
                return None

            self._emit_progress(f"Loaded feedback data with {len(feedback_df)} rows")

            # Get total rows in input file for progress tracking
            total_rows = sum(1 for _ in open(lmd_path, 'r', encoding='utf-8')) - 1  # Subtract header
            self._emit_progress(f"Processing {total_rows} rows in chunks of {chunk_size}")

            # Process in chunks
            processed_chunks = []
            offset = 0

            while offset < total_rows:
                # Read chunk
                chunk_df = pl.read_csv(
                    lmd_path,
                    skip_rows=offset + 1,  # +1 to skip header
                    n_rows=min(chunk_size, total_rows - offset),
                    has_header=False,
                    ignore_errors=True,
                    infer_schema_length=0
                )

                if chunk_df.is_empty():
                    break

                # Add column names from first chunk
                if offset == 0:
                    # Read header to get column names
                    header_df = pl.read_csv(lmd_path, n_rows=0)
                    chunk_df = chunk_df.rename({old: new for old, new in zip(chunk_df.columns, header_df.columns)})

                self._emit_progress(f"Processing chunk {len(processed_chunks) + 1}: rows {offset + 1}-{min(offset + chunk_size, total_rows)}")

                # Process this chunk with feedback data
                processed_chunk = self._process_client_feedback_data_polars(chunk_df, feedback_df)

                if processed_chunk is not None:
                    processed_chunks.append(processed_chunk)

                offset += chunk_size

            if not processed_chunks:
                self._emit_progress("ERROR: No chunks were processed successfully")
                return None

            # Concatenate all processed chunks
            self._emit_progress("Combining processed chunks...")
            final_df = pl.concat(processed_chunks, how='vertical')

            self._emit_progress(f"Client feedback processing completed. Total rows: {len(final_df)}")
            return final_df

        except Exception as e:
            error_msg = f"Client feedback processing with chunking failed: {str(e)}"
            logger.error(f"{error_msg}\n{e}", exc_info=True)
            self._emit_progress(error_msg)
            return None

    def _process_client_feedback_data_polars(self, result_df: pl.DataFrame, feedback_df: pl.DataFrame) -> Optional[pl.DataFrame]:
        """Process client feedback data using Polars vectorized operations."""
        try:
            # Store original columns for preservation
            original_columns = result_df.columns.copy()

            # Find Road ID column variants in input data
            road_id_variants = ['Road ID', 'RoadID', 'road_id', 'roadid', 'ROADID', 'Road_ID', 'road ID']
            input_road_col = None

            for variant in road_id_variants:
                if variant in result_df.columns:
                    input_road_col = variant
                    break

            if not input_road_col:
                logger.error("No Road ID column found in input data")
                return result_df

            # Find chainage column in input data
            chainage_variants = ['Chainage', 'chainage', 'CHAINAGE', 'Location', 'location']
            input_chainage_col = None

            for variant in chainage_variants:
                if variant in result_df.columns:
                    input_chainage_col = variant
                    break

            if not input_chainage_col:
                logger.error("No Chainage column found in input data")
                return result_df

            # Find Road ID column in feedback data
            feedback_road_col = None
            for variant in road_id_variants:
                if variant in feedback_df.columns:
                    feedback_road_col = variant
                    break

            if not feedback_road_col:
                logger.error("No Road ID column found in feedback data")
                return result_df

            # Find chainage columns in feedback data (start and end)
            start_chainage_variants = ['Start Chainage', 'start_chainage', 'StartChainage', 'start chainage', 'Start Chainage (km)']
            end_chainage_variants = ['End Chainage', 'end_chainage', 'EndChainage', 'end chainage', 'End Chainage (km)']

            start_col = None
            end_col = None

            for variant in start_chainage_variants:
                if variant in feedback_df.columns:
                    start_col = variant
                    break

            for variant in end_chainage_variants:
                if variant in feedback_df.columns:
                    end_col = variant
                    break

            if not start_col or not end_col:
                logger.error(f"Required chainage columns not found in feedback data. Available: {feedback_df.columns}")
                return result_df

            logger.info(f"Using columns - Feedback Road ID: '{feedback_road_col}', Input Road ID: '{input_road_col}', Input Chainage: '{input_chainage_col}', Feedback Start: '{start_col}', Feedback End: '{end_col}'")

            # Process feedback data with Polars
            feedback_processed = feedback_df.with_columns([
                # Convert chainage from km to meters and round to nearest 10
                (((pl.col(start_col).cast(pl.Float64) * 1000) / 10).round(0) * 10).alias('start_chainage_m'),
                (((pl.col(end_col).cast(pl.Float64) * 1000) / 10).round(0) * 10).alias('end_chainage_m')
            ])

            # Prepare input data with proper chainage conversion
            result_processed = result_df.with_columns([
                # Convert chainage to meters if it's from 'location' column (PAS files)
                pl.when(pl.lit(input_chainage_col.lower() == 'location'))
                .then(pl.col(input_chainage_col).cast(pl.Float64) * 1000)
                .otherwise(pl.col(input_chainage_col).cast(pl.Float64))
                .alias('chainage_m'),

                # Ensure road ID is numeric for comparison
                pl.col(input_road_col).cast(pl.Float64).alias('road_id_numeric')
            ])

            # Prepare feedback with numeric road ID
            feedback_final = feedback_processed.with_columns([
                pl.col(feedback_road_col).cast(pl.Float64).alias('fb_road_id_numeric')
            ])

            feedback_columns_to_add = ['Site Description', 'Treatment 2024', 'Treatment 2025', 'Treatment 2026', 'Terminal', 'Foamed Bitumen %', 'Cement %', 'Lime %']

            # Apply feedback data using a more efficient approach
            self._emit_progress("Matching feedback data with input records...")

            # Add feedback columns with default empty values
            for col in feedback_columns_to_add:
                result_processed = result_processed.with_columns([
                    pl.lit("").alias(col)
                ])

            # For each input row, find matching feedback data
            feedback_updates = {col: [] for col in feedback_columns_to_add}

            for row in result_processed.iter_rows(named=True):
                road_id = row['road_id_numeric']
                chainage = row['chainage_m']

                # Find matching feedback record
                matching_feedback = None
                for fb_row in feedback_final.iter_rows(named=True):
                    if (fb_row['fb_road_id_numeric'] == road_id and
                        fb_row['start_chainage_m'] <= chainage <= fb_row['end_chainage_m']):
                        matching_feedback = fb_row
                        break

                # Update feedback columns if match found
                for col in feedback_columns_to_add:
                    if matching_feedback and col in matching_feedback:
                        feedback_updates[col].append(matching_feedback[col])
                    else:
                        feedback_updates[col].append("")

            # Apply all updates at once
            update_exprs = []
            for col in feedback_columns_to_add:
                update_exprs.append(pl.Series(col, feedback_updates[col]).alias(col))

            result_processed = result_processed.with_columns(update_exprs)

            # Create final dataframe with preserved column order
            select_exprs = []
            for col in original_columns:
                select_exprs.append(pl.col(col))

            # Add the new feedback columns at the end
            for col in feedback_columns_to_add:
                select_exprs.append(pl.col(col))

            final_df = result_processed.select(select_exprs)

            # Format TestDateUTC back to original string format if it exists
            if 'TestDateUTC' in final_df.columns:
                if final_df['TestDateUTC'].dtype in [pl.Datetime, pl.Datetime('ns'), pl.Datetime('us')]:
                    select_exprs = []
                    for col in final_df.columns:
                        if col == 'TestDateUTC':
                            select_exprs.append(
                                pl.col('TestDateUTC').dt.strftime('%d/%m/%Y %H:%M:%S%.3f').alias('TestDateUTC')
                            )
                        else:
                            select_exprs.append(pl.col(col))
                    final_df = final_df.select(select_exprs)

            matches_found = sum(1 for col in feedback_columns_to_add if final_df[col].n_unique() > 1)
            logger.info(f"Client feedback processing completed. Added {len(feedback_columns_to_add)} columns")

            return final_df

        except Exception as e:
            logger.error(f"Client feedback processing failed: {e}")
            return result_df

            # Find Road ID column variants in input data
            road_id_variants = ['Road ID', 'RoadID', 'road_id', 'roadid', 'ROADID', 'Road_ID', 'road ID']
            input_road_col = None

            for variant in road_id_variants:
                if variant in result_df.columns:
                    input_road_col = variant
                    break

            if not input_road_col:
                logger.error("No Road ID column found in input data")
                return result_df

            # Find chainage column in input data
            chainage_variants = ['Chainage', 'chainage', 'CHAINAGE', 'Location', 'location']
            input_chainage_col = None

            for variant in chainage_variants:
                if variant in result_df.columns:
                    input_chainage_col = variant
                    break

            if not input_chainage_col:
                logger.error("No Chainage column found in input data")
                return result_df

            # Find Road ID column in feedback data
            feedback_road_col = None
            for variant in road_id_variants:
                if variant in feedback_df.columns:
                    feedback_road_col = variant
                    break

            if not feedback_road_col:
                logger.error("No Road ID column found in feedback data")
                return result_df

            # Find chainage columns in feedback data (start and end)
            start_chainage_variants = ['Start Chainage', 'start_chainage', 'StartChainage', 'start chainage', 'Start Chainage (km)']
            end_chainage_variants = ['End Chainage', 'end_chainage', 'EndChainage', 'end chainage', 'End Chainage (km)']

            start_col = None
            end_col = None

            for variant in start_chainage_variants:
                if variant in feedback_df.columns:
                    start_col = variant
                    break

            for variant in end_chainage_variants:
                if variant in feedback_df.columns:
                    end_col = variant
                    break

            if not start_col or not end_col:
                logger.error(f"Required chainage columns not found in feedback data. Available: {feedback_df.columns}")
                return result_df

            logger.info(f"Using columns - Feedback Road ID: '{feedback_road_col}', Input Road ID: '{input_road_col}', Input Chainage: '{input_chainage_col}', Feedback Start: '{start_col}', Feedback End: '{end_col}'")

            # Process feedback data with Polars
            feedback_processed = feedback_df.with_columns([
                # Convert chainage from km to meters and round to nearest 10
                (((pl.col(start_col).cast(pl.Float64) * 1000) / 10).round(0) * 10).alias('start_chainage_m'),
                (((pl.col(end_col).cast(pl.Float64) * 1000) / 10).round(0) * 10).alias('end_chainage_m')
            ])

            # Prepare input data with proper chainage conversion
            result_processed = result_df.with_columns([
                # Convert chainage to meters if it's from 'location' column (PAS files)
                pl.when(pl.lit(input_chainage_col.lower() == 'location'))
                .then(pl.col(input_chainage_col).cast(pl.Float64) * 1000)
                .otherwise(pl.col(input_chainage_col).cast(pl.Float64))
                .alias('chainage_m'),

                # Ensure road ID is numeric for comparison
                pl.col(input_road_col).cast(pl.Float64).alias('road_id_numeric')
            ])

            # Prepare feedback with numeric road ID
            feedback_final = feedback_processed.with_columns([
                pl.col(feedback_road_col).cast(pl.Float64).alias('fb_road_id_numeric')
            ])

            feedback_columns_to_add = ['Site Description', 'Treatment 2024', 'Treatment 2025', 'Treatment 2026', 'Terminal', 'Foamed Bitumen %', 'Cement %', 'Lime %']

            # Apply feedback data using a more efficient approach
            self._emit_progress("Matching feedback data with input records...")

            # Add feedback columns with default empty values
            for col in feedback_columns_to_add:
                result_processed = result_processed.with_columns([
                    pl.lit("").alias(col)
                ])

            # For each input row, find matching feedback data
            feedback_updates = {col: [] for col in feedback_columns_to_add}

            for row in result_processed.iter_rows(named=True):
                road_id = row['road_id_numeric']
                chainage = row['chainage_m']

                # Find matching feedback record
                matching_feedback = None
                for fb_row in feedback_final.iter_rows(named=True):
                    if (fb_row['fb_road_id_numeric'] == road_id and
                        fb_row['start_chainage_m'] <= chainage <= fb_row['end_chainage_m']):
                        matching_feedback = fb_row
                        break

                # Update feedback columns if match found
                for col in feedback_columns_to_add:
                    if matching_feedback and col in matching_feedback:
                        feedback_updates[col].append(matching_feedback[col])
                    else:
                        feedback_updates[col].append("")

            # Apply all updates at once
            update_exprs = []
            for col in feedback_columns_to_add:
                update_exprs.append(pl.Series(col, feedback_updates[col]).alias(col))

            result_processed = result_processed.with_columns(update_exprs)

            # Create final dataframe with preserved column order
            select_exprs = []
            for col in original_columns:
                select_exprs.append(pl.col(col))

            # Add the new feedback columns at the end
            for col in feedback_columns_to_add:
                select_exprs.append(pl.col(col))

            final_df = result_processed.select(select_exprs)

            # Format TestDateUTC back to original string format if it exists
            if 'TestDateUTC' in final_df.columns:
                if final_df['TestDateUTC'].dtype in [pl.Datetime, pl.Datetime('ns'), pl.Datetime('us')]:
                    select_exprs = []
                    for col in final_df.columns:
                        if col == 'TestDateUTC':
                            select_exprs.append(
                                pl.col('TestDateUTC').dt.strftime('%d/%m/%Y %H:%M:%S%.3f').alias('TestDateUTC')
                            )
                        else:
                            select_exprs.append(pl.col(col))
                    final_df = final_df.select(select_exprs)

            matches_found = sum(1 for col in feedback_columns_to_add if final_df[col].n_unique() > 1)
            logger.info(f"Client feedback processing completed. Added {len(feedback_columns_to_add)} columns")

            return final_df

        except Exception as e:
            logger.error(f"Client feedback processing failed: {e}")
            return result_df